{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8e87b7aad4cb4fd2a9dffda4339ba11d",
     "grade": false,
     "grade_id": "cell-a23b14ebbe230e18",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Information Visualization II\n",
    "\n",
    "## School of Information, University of Michigan\n",
    "\n",
    "## Week 4:\n",
    "\n",
    "- Text visualizations\n",
    "\n",
    "## Assignment Overview\n",
    "### The objectives for this week are for you to:\n",
    "\n",
    "- Understand how to model a corpus using statistical and visual techniques\n",
    "- Construct an interactive information visualization for search tasks\n",
    "\n",
    "### The total score of this assignment will be\n",
    "- Problem 1 (20 points)\n",
    "- Problem 2 (80 points)\n",
    "\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- We have created two textual datasets for you. One contains the text from Wikipedia pages related to data mining (algorithms, software, techniques, people, etc.). The second is related to *real* mining (equipment, companies, locations, etc.). \n",
    "\n",
    "### Important notes:\n",
    "1) Grading for this assignment is entirely done by manual inspection. You will have lots of control over the look and feel of problem 2.\n",
    "\n",
    "2) When turning in your PDF, please use the File -> Print -> Save as PDF option ***from your browser***. Do ***not*** use the File->Download as->PDF option. Complete instructions for this are under Resources in the Coursera page for this class.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "import spacy\n",
    "import math\n",
    "import numpy as np\n",
    "import scattertext as st\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from altair import datum\n",
    "sp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some utilitity classes that will help us load the data in\n",
    "def lemmatize(instring,title=\"\",lemmaCache = {}):\n",
    "    parsed = None\n",
    "    \n",
    "    if ((title != \"\") & (title in lemmaCache)):\n",
    "        parsed = lemmaCache[title]    \n",
    "    else:\n",
    "        parsed = sp(instring)\n",
    "\n",
    "    if (lemmaCache != None):\n",
    "        lemmaCache[title] = parsed\n",
    "    sent = [x.text if (x.lemma_ == \"-PRON-\") else x.lemma_ for x in parsed]\n",
    "    return(sent)\n",
    "\n",
    "def generateData(filepath,lemmaCache=None):\n",
    "    articles = []\n",
    "    with open(filepath) as fp:\n",
    "        for docid, line in enumerate(fp):\n",
    "            doc = json.loads(line)\n",
    "            doclines = doc['text'].split(\"\\n\\n\")\n",
    "            for lineid,docline in enumerate(doclines):\n",
    "                obj = {}\n",
    "                obj['docid'] = docid;\n",
    "                obj['title'] = doc['title']\n",
    "                obj['lineid'] = lineid\n",
    "                paraterms = lemmatize(docline,doc['title']+str(lineid),lemmaCache)\n",
    "                obj['text'] = ' ' + ' '.join(paraterms) + ' '\n",
    "                obj['tokencount'] = len(paraterms)\n",
    "                if ('category' in doc):\n",
    "                    obj['category'] = doc['category']\n",
    "                if (len(paraterms) > 10):\n",
    "                    articles.append(obj)\n",
    "    return pd.DataFrame(articles)\n",
    "\n",
    "def loadFile(classname,classpath,maxc=200,lemmaCache={}):\n",
    "    articles = []\n",
    "    with open(classpath) as fp:\n",
    "        for docid, line in enumerate(fp):\n",
    "            doc = json.loads(line)\n",
    "            doclines = doc['text'].split(\"\\n\\n\")\n",
    "            obj = {}\n",
    "            obj['docid'] = docid;\n",
    "            obj['title'] = doc['title']\n",
    "            paraterms = lemmatize(doc['text'],doc['title'],lemmaCache)\n",
    "            obj['text'] = ' ' + ' '.join(paraterms) + ' '\n",
    "            obj['label'] = classname\n",
    "            if ('category' in doc):\n",
    "                obj['category'] = doc['category']\n",
    "            if (len(paraterms) > 10):\n",
    "                articles.append(obj)\n",
    "            if (docid > maxc):\n",
    "                break\n",
    "    return(articles)\n",
    "\n",
    "def loadClasses(class1name,class1path,class2name,class2path,maxc=300):\n",
    "    articles = loadFile(class1name,class1path) + loadFile(class2name,class2path)\n",
    "    return pd.DataFrame(articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataTransformerRegistry.enable('json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# enable correct rendering\n",
    "alt.renderers.enable('default')\n",
    "\n",
    "# uses intermediate json files to speed things up\n",
    "alt.data_transformers.enable('json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "acb5d3a712c6254a51b5e9b1476aa91b",
     "grade": false,
     "grade_id": "cell-1e49deb9034c21c1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Before we start...\n",
    "\n",
    "We have created a function for you called ```lemmatize(...)```. It takes as input a string and assumes that spaces are token delimiters. For each token/word, the system will lowercase it, stem it (getting the root), and generally clean it up.  The data we load from our files undergoes the same transformation. So it's important to lemmatize your terms if you are looking them up. For example, you won't find the word \"data\" in the DataFrame. All instances get transformed to \"datum.\" Thus, it's important to remember to do this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemmatized version of Data Mining is: ['data', 'mining']\n",
      "The lemmatized version of executing awesome algorithms is: ['execute', 'awesome', 'algorithm']\n"
     ]
    }
   ],
   "source": [
    "# here's a few examples\n",
    "\n",
    "query1 = \"Data Mining\"\n",
    "print(\"The lemmatized version of \"+query1+\" is:\", lemmatize(query1))\n",
    "\n",
    "query1 = \"executing awesome algorithms\"\n",
    "print(\"The lemmatized version of \"+query1+\" is:\", lemmatize(query1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d3930089745fdbb105f08345ba29d83f",
     "grade": false,
     "grade_id": "cell-980fb7d6b73f6dc8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Problem 1 (20 points)\n",
    "\n",
    "For this first problem, we will be comparing which terms most often appear in which of our two corpora: 'data' mining and 'real' mining.\n",
    "\n",
    "![\"mining v mining](assets/miningvsmining.png)\n",
    "\n",
    "Let's load the data in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will load the two files and label them with one of the two class labels\n",
    "# lemmatizing these files takes some time on Coursera so we've pre-calculated it for you.\n",
    "# If you want to run this process, uncomment the next two lines of code\n",
    "\n",
    "miningdf = loadClasses('data mining','assets/mlarticles.jsonl','real mining','assets/miningarticles.jsonl')\n",
    "miningdf.to_csv(\"assets/miningvmining.csv\",index=False)\n",
    "\n",
    "# load from cached file\n",
    "miningdf = pd.read_csv(\"assets/miningvmining.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>189</td>\n",
       "      <td>KNIME</td>\n",
       "      <td>knime ( ) , the konstanz information miner , ...</td>\n",
       "      <td>data mining</td>\n",
       "      <td>Data mining and machine learning software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Wolfram Mathematica</td>\n",
       "      <td>wolfram mathematica ( usually term mathematic...</td>\n",
       "      <td>data mining</td>\n",
       "      <td>Data mining and machine learning software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>126</td>\n",
       "      <td>Eager learning</td>\n",
       "      <td>in artificial intelligence , eager learning b...</td>\n",
       "      <td>data mining</td>\n",
       "      <td>Machine learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>144</td>\n",
       "      <td>Silesian-American Corporation</td>\n",
       "      <td>silesian - american corporation ( saco ) be r...</td>\n",
       "      <td>real mining</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>21</td>\n",
       "      <td>Broken Hill</td>\n",
       "      <td>broken hill be an inland mining city in the f...</td>\n",
       "      <td>real mining</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     docid                          title  \\\n",
       "189    189                          KNIME   \n",
       "5        5            Wolfram Mathematica   \n",
       "126    126                 Eager learning   \n",
       "346    144  Silesian-American Corporation   \n",
       "223     21                    Broken Hill   \n",
       "\n",
       "                                                  text        label  \\\n",
       "189   knime ( ) , the konstanz information miner , ...  data mining   \n",
       "5     wolfram mathematica ( usually term mathematic...  data mining   \n",
       "126   in artificial intelligence , eager learning b...  data mining   \n",
       "346   silesian - american corporation ( saco ) be r...  real mining   \n",
       "223   broken hill be an inland mining city in the f...  real mining   \n",
       "\n",
       "                                      category  \n",
       "189  Data mining and machine learning software  \n",
       "5    Data mining and machine learning software  \n",
       "126                           Machine learning  \n",
       "346                                        NaN  \n",
       "223                                        NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at what's inside.  We have a document id (docid), title (from Wikipedia)\n",
    "# the text, the label (one of: 'real mining' or 'data mining'), and a category column\n",
    "# which you can ignore for now (it has the Wikipedia category for just the data mining articles)\n",
    "\n",
    "miningdf.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' matlab ( matrix laboratory ) be a multi - paradigm numerical computing environment and proprietary programming language develop by mathworks . matlab allow matrix manipulation , plot of function and datum , implementation of algorithm , creation of user interface , and interfac with program write in other language , include c , c++ , c # , java , fortran and python . \\n\\n although matlab be intend primarily for numerical computing , an optional toolbox use the mupad symbolic engine , allow access to symbolic computing ability . an additional package , simulink , add graphical multi - domain simulation and model - base design for dynamic and embed system . \\n\\n as of 2018 , matlab have more than 3 million user worldwide . matlab user come from various background of engineering , science , and economic . '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you'll notice that the text is lemmatized. Here's the text for the first entry:\n",
    "miningdf.head(1).text.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a82073a71f5e62877e6f77691c35b1ae",
     "grade": false,
     "grade_id": "cell-b9871f6c682c6fcd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We will be using the [scattertext](https://github.com/JasonKessler/scattertext) library to create, analyze and position the terms. We encourage you to take a look at all the features of scattertext. It has a lot of \"knobs\" to control the analysis. It will also create a very fancy Web-based, interactive visualization for you if you want. For our initial experiment, we're going to start simple. We simply want to plots terms based on how common they are in 'data mining' and in 'real mining.' The lower-left corner will hold uncommon terms for both. The upper right will be terms that  often appear in both domains. A way to think of this is that terms on the diagonal (slope 1) appear equally in both domains. The other two corners are the outliers--these are terms that are either more common for data or real mining.  Here's a screenshot of what we'll get:\n",
    "\n",
    "![\"scattertext\"](assets/scattertext_small.png)\n",
    "\n",
    "[Click here](assets/scattertext.png) for a larger image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "84b77eb79d505d68907b298d857bac01",
     "grade": false,
     "grade_id": "cell-24f7e1b5c120bfe0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We're going to run the analysis for you and have you generate the visualization. Once you get the first version working, you can play with the options to see how they impact the analysis/visualization. In particular, you might want to change the term frequency and PMI (pointwise mutual information) thresholds to see what they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the scattertext analysis pipeline to the text, this will create a new column called parse\n",
    "miningdf = miningdf.assign(\n",
    "    parse=lambda df: df.text.apply(st.whitespace_nlp_with_sentences)\n",
    ")\n",
    "\n",
    "# create a \"corpus\" object\n",
    "corpus = st.CorpusFromParsedDocuments(\n",
    "    # use the miningdf as input. The category col is \"label\" and the parsed data is in \"parse\"\n",
    "    miningdf, category_col='label', parsed_col='parse'\n",
    "    # the unigram corpus means we want single words (there's another version that gets throws out stopwords)\n",
    "    # the association compactor says we want the 2000 most label-associated terms\n",
    ").build().get_unigram_corpus().compact(st.AssociationCompactor(2000))\n",
    "\n",
    "# next, we build the actual visualization \n",
    "scatterdata = st.produce_scattertext_explorer(\n",
    "    corpus,                            # the corpus\n",
    "    category='data mining',            # the \"base\" category\n",
    "    category_name='data mining',       # the label for the category (same in this case)\n",
    "    not_category_name='real mining',   # the label of the other category\n",
    "    minimum_term_frequency=0,          # threshold frequency\n",
    "    pmi_threshold_coefficient=0,       # the PMI threshold\n",
    "    return_data=True,                  # this tells scattertext to return the data rather than saving an HTML page\n",
    "    transform=st.Scalers.dense_rank    # where to place identically ranked terms (on top of each other here)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "26d284ec5da60782e68d35a62ac8406a",
     "grade": false,
     "grade_id": "cell-b850158a74fa5abb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "At this point, ```scatterdata``` will contain all kinds of information. For example, ```scatterdata['info']['category_terms']``` will give you the terms most related to the \"category\" (remember, this is data mining). In contrast, you can get the \"real mining\" terms using not_category_terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terms most associated with data mining  ['datum', 'learning', 'algorithm', 'analysis', 'computer', 'data', 'model', 'machine', 'research', 'pattern'] \n",
      "\n",
      "terms most associated with real mining  ['mine', 'town', 'gold', 'south', 'coal', 'locate', 'miner', 'diamond', 'river', 'north']\n"
     ]
    }
   ],
   "source": [
    "print(\"terms most associated with data mining \",scatterdata['info']['category_terms'],\"\\n\")\n",
    "print(\"terms most associated with real mining \",scatterdata['info']['not_category_terms'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e719d3be4caea1fe48d1ac3691ea05ce",
     "grade": false,
     "grade_id": "cell-67dff7c7ad87580f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The more important piece for our visualization purposes is the \"data\" part of scatterdata. This is a list of \"objects,\" one for each term. For example:\n",
    "\n",
    "![\"data example\"](assets/scattertext_content.png)\n",
    "\n",
    "This is the first item in the data list. There are a number of fields here. You can look at the documentation for scattertext for the details. The only items we care about right now will be ```x```, ```y```, ```term```, and ```s```. These respectively tell us the x/y coordinate for the term, the term itself, and the \"distance\" of the term from the central line (slope 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 0.0,\n",
       " 'y': 0.13265306122448978,\n",
       " 'ox': 0.0,\n",
       " 'oy': 0.13265306122448978,\n",
       " 'term': 'matlab',\n",
       " 'cat25k': 15,\n",
       " 'ncat25k': 0,\n",
       " 'neut25k': 0,\n",
       " 'neut': 0,\n",
       " 'extra25k': 0,\n",
       " 'extra': 0,\n",
       " 'cat': 13,\n",
       " 'ncat': 0,\n",
       " 's': 0.8930722891566265,\n",
       " 'os': 0.12921901946292189,\n",
       " 'bg': 1.1352105640948616e-05}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scatterdata['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "50f5b0355dff05e926bbbe8aecef4647",
     "grade": false,
     "grade_id": "cell-5e4933e0a6bb891a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We would like for you to use this data to generate a visualization as in the example below. You're welcome to try to make it fancier, but consider this the minimum solution (notice the tooltips and colors):\n",
    "\n",
    "![\"scattertext example\"](assets/interactive_scatter.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0d37d9ef17b384fca50661125e46f581",
     "grade": true,
     "grade_id": "cell-187c23a630244666",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def genScattertext():\n",
    "    # this function should return an Altair chart as specified above\n",
    "    \n",
    "   \n",
    "    \n",
    "    source = pd.DataFrame(scatterdata['data'])\n",
    "    \n",
    "    toret = alt.Chart(source).mark_circle().encode(\n",
    "    alt.X('x:Q', scale=alt.Scale(zero=False), axis=alt.Axis(title='⏪ less physical | more physical ⏩')),\n",
    "    alt.Y('y:Q', scale=alt.Scale(zero=False), axis=alt.Axis(title='⏪ less data | more data ⏩')),\n",
    "    color=alt.Color( \"s:Q\",scale=alt.Scale(scheme=\"redblue\"))\n",
    ").properties(height=600, width=600)\n",
    "    \n",
    "    \n",
    "    toret = toret.encode(\n",
    "    tooltip='term'\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    return(toret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-c6051451f6a849e9a52fb265eae386cd\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-c6051451f6a849e9a52fb265eae386cd\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-c6051451f6a849e9a52fb265eae386cd\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"url\": \"altair-data-62d1f9a1ff2a83985455e820854a7395.json\", \"format\": {\"type\": \"json\"}}, \"mark\": \"circle\", \"encoding\": {\"color\": {\"type\": \"quantitative\", \"field\": \"s\", \"scale\": {\"scheme\": \"redblue\"}}, \"tooltip\": {\"type\": \"nominal\", \"field\": \"term\"}, \"x\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"\\u23ea less physical | more physical \\u23e9\"}, \"field\": \"x\", \"scale\": {\"zero\": false}}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"\\u23ea less data | more data \\u23e9\"}, \"field\": \"y\", \"scale\": {\"zero\": false}}}, \"height\": 600, \"width\": 600, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\"}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if your code above works correctly, this should generate the plot\n",
    "genScattertext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "46cd2c652a34aee8595922010272ef46",
     "grade": false,
     "grade_id": "cell-51609ecddabf3a93",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Problem 2\n",
    "For this problem, we would like for you to build a visual query system using tilebars! You will need to build a function that returns an Altair visualization. It will take as input a query (text string), an option to normalize the data or not (A boolean True or False), and a string indicating the sort order (\"name\" or \"score\"). If you build your interface correctly, you will hopefully get something like the following:\n",
    "\n",
    "![\"tilebar example\"](assets/tilebar.gif)\n",
    "\n",
    "You are welcome to come up with your own style, add interactivity, decide how to normalize and score, the data, etc. This problem is very open ended. If you don't remember how a tilebar is created, now is a good time to go back to the lecture and watch the video.\n",
    "\n",
    "Before we get started, let's load the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're only working with data mining here\n",
    "\n",
    "\n",
    "# lemmatizing these files takes some time on Coursera so we've pre-calculated it for you.\n",
    "# If you want to run this process, uncomment the next three lines of code\n",
    "# We're going to \"cache\" lemmas to speed up some operations\n",
    "\n",
    "lemmaCache = {}\n",
    "dataminingdf = generateData('assets/mlarticles.jsonl',lemmaCache)\n",
    "dataminingdf.to_csv('assets/mlarticles.csv',index=False)\n",
    "\n",
    "dataminingdf = pd.read_csv('assets/mlarticles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>title</th>\n",
       "      <th>lineid</th>\n",
       "      <th>text</th>\n",
       "      <th>tokencount</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MATLAB</td>\n",
       "      <td>0</td>\n",
       "      <td>matlab ( matrix laboratory ) be a multi - par...</td>\n",
       "      <td>64</td>\n",
       "      <td>Data mining and machine learning software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>MATLAB</td>\n",
       "      <td>1</td>\n",
       "      <td>although matlab be intend primarily for numer...</td>\n",
       "      <td>48</td>\n",
       "      <td>Data mining and machine learning software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>MATLAB</td>\n",
       "      <td>2</td>\n",
       "      <td>as of 2018 , matlab have more than 3 million ...</td>\n",
       "      <td>27</td>\n",
       "      <td>Data mining and machine learning software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Ray Kurzweil</td>\n",
       "      <td>0</td>\n",
       "      <td>raymond kurzweil ( ; bear february 12 , 1948 ...</td>\n",
       "      <td>105</td>\n",
       "      <td>Machine learning researchers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Ray Kurzweil</td>\n",
       "      <td>1</td>\n",
       "      <td>kurzweil receive the 1999 national medal of t...</td>\n",
       "      <td>141</td>\n",
       "      <td>Machine learning researchers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   docid         title  lineid  \\\n",
       "0      0        MATLAB       0   \n",
       "1      0        MATLAB       1   \n",
       "2      0        MATLAB       2   \n",
       "3      1  Ray Kurzweil       0   \n",
       "4      1  Ray Kurzweil       1   \n",
       "\n",
       "                                                text  tokencount  \\\n",
       "0   matlab ( matrix laboratory ) be a multi - par...          64   \n",
       "1   although matlab be intend primarily for numer...          48   \n",
       "2   as of 2018 , matlab have more than 3 million ...          27   \n",
       "3   raymond kurzweil ( ; bear february 12 , 1948 ...         105   \n",
       "4   kurzweil receive the 1999 national medal of t...         141   \n",
       "\n",
       "                                    category  \n",
       "0  Data mining and machine learning software  \n",
       "1  Data mining and machine learning software  \n",
       "2  Data mining and machine learning software  \n",
       "3               Machine learning researchers  \n",
       "4               Machine learning researchers  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look at the first few lines\n",
    "dataminingdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d1efe486b331e27e6484c9101033c3e4",
     "grade": false,
     "grade_id": "cell-5498c6547822cdbd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "What you see above is a row for every document and every \"line.\"  In pre-processing the top section of each Wikipedia article for you. We have taken each paragraph and made it into a new line. For example, the [MATLAB](https://en.wikipedia.org/wiki/MATLAB) article has 3 lines. The frame has a document id (docid), title, line id (lineid -- based on the order the line appears), text (the text of the line), tokencount (the number of words in that line), and the category of the article (which you can use if you want)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7536666dc826334fab8c31768316ca53",
     "grade": false,
     "grade_id": "cell-e3fa74f5bb3e9f80",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "At this point we're going to start implementing the ```drawTilebars``` function.\n",
    "\n",
    "As we mentioned above, everything outside of the basic functions and tilebar encoding is fair game. You can decide how to rank documents (do you want to do it based on whether the matches are in the same line? whether there are many matches throughout the article?). You can also decide how you want to implement normalization on the tilebars themselves (by tokens in the line? by the maximum times the term appears in the document? the maximum time it appears in all documents?). Here's a static screenshot of our tilebars for \"clustering techniques\" normalized with score based ordering:\n",
    "\n",
    "![\"tilebar example\"](assets/clustering_technique.png)\n",
    "\n",
    "***Again, you are not expected to reverse engineer our solution.***\n",
    "\n",
    "Some hints:\n",
    "* Take a look at some of the pandas features for text analysis. For example, for a row in our dataframe, you can get the count of the number of times a specific token appears by doing ```row['text'].count(' ' + term + ' ')```\n",
    "* You likely want to calculate two things--one is a dataframe describing your tilebar information, the other is some kind of document order. If you're clever, you can do it all at once. A less efficient solution might require two passes.\n",
    "* Think about what you need to know in order to encode the \"cell\" of the tilebar. Your dataframe should contain that data.\n",
    "* Consider the look and feel of your solution. We will be considering the aesthetic choices you are making. \n",
    "* Think through how to build the \"small multiples\" here. You can use combinations of concatenation, faceting, and repeated charts. You'll likely need to play with a few solutions to get the look you're happy with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4e2adda8f3c5972e3fddac7e989e6c1a",
     "grade": true,
     "grade_id": "cell-7ae43d9e0f6a804a",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def drawTilebars(query,normalized=False,sortby='title'):\n",
    "    # this function takes \n",
    "    # query: a string query\n",
    "    # normalized: an argument about whether to normalize the tilebar (True or False)\n",
    "    #   if false, the the color of the tile should map to the count\n",
    "    #   if true, you should decide how you want to normalize (by the max count overall? max count in article?)\n",
    "    # sortby: a string of either \"title\" or \"score\"\n",
    "    #   if title, the tilebars should be returned based on alphabetical order of the articles\n",
    "    #   if score, you can decide how you want to rank the articles\n",
    "    # the function returns: an altair chart\n",
    "    print(\"the lemmatized query terms are: \",lemmatize(query))\n",
    "    print(\"nomalized is \",normalized)\n",
    "    print(\"I will sort by\", sortby)\n",
    "    \n",
    "    sortOrder = None\n",
    "    \n",
    "    lemmatized_query_list = lemmatize(query)\n",
    "    \n",
    "    \n",
    "    dataframe_copy =  pd.read_csv('assets/mlarticles.csv')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    value_columns = []\n",
    "    for item in lemmatized_query_list:\n",
    "        if normalized == False:\n",
    "            i = 'count of item ' + item\n",
    "        elif normalized == True:\n",
    "            i = 'normalized count of item ' + item \n",
    "        dataframe_copy[item] = item\n",
    "        dataframe_copy['count of item ' + item] =  dataframe_copy['text'].str.count(item)\n",
    "        dataframe_copy['normalized count of item ' + item ] =  dataframe_copy['count of item ' + item]/dataframe_copy['tokencount']\n",
    "        value_columns.append(i)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    dataframe_melted = pd.melt(dataframe_copy, id_vars =['title', 'lineid'], value_vars = value_columns)\n",
    "    dataframe_melted['name'] = dataframe_melted['variable'].str.split().str[-1] \n",
    "    \n",
    "    \n",
    "    if sortby == 'title':\n",
    "        sortOrder = sorted(dataframe_melted.title.unique())\n",
    "    else:\n",
    "        sortOrder = list(dataframe_melted.groupby('title')['value'].sum().sort_values(ascending=False).index)\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    if normalized == False:\n",
    "        colorChoice = alt.Color('value:Q', scale=alt.Scale(scheme='blues'), legend=None)\n",
    "    else:\n",
    "        colorChoice = alt.Color('value:Q', scale=alt.Scale(scheme='reds'), legend=None)\n",
    "    \n",
    "    chart = alt.Chart(dataframe_melted).mark_rect().encode(\n",
    "    x = alt.X('lineid:O', title=None, axis=alt.Axis(grid=False, labels=False)), \n",
    "    y = alt.Y('name:N', title=None, axis=alt.Axis(grid=False)),\n",
    "    color = colorChoice\n",
    ").transform_filter(datum.value > 0).facet(row=alt.Row('title', title=None, header=alt.Header(labelOrient='right', labelAngle=0, labelFontWeight='bold', labelFontSize=12),sort=sortOrder))\n",
    " \n",
    "    return(chart)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f9421e56d7a5a82a0cb85b20ce2a57b3",
     "grade": false,
     "grade_id": "cell-4eeb0e4261e55e16",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "If you built your solution correctly, you should be able to simply run the code below. Note that we don't use Altair interactivity because we don't know how you chose to implement your solution. The visualization will likely flicker as you recalculate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32c6645a1a8a4ae8ad864c605fb5e526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HBox(children=(Text(value='', description='Query:'), Button(description='Se…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec10ca60571c4ee8beb1061a8f9ca4ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = widgets.Output()\n",
    "from IPython.display import display\n",
    "\n",
    "def clicked(b):\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        _norm = True\n",
    "        _sortby = 'name'\n",
    "        _query = querybox.value\n",
    "        \n",
    "        if (normalizedradio.value == \"false\"):\n",
    "            _norm = False\n",
    "            \n",
    "        if (sortradio.value == 'score'):\n",
    "            _sortby = 'score'\n",
    "            \n",
    "        if (_query == \"\"):\n",
    "            print(\"please enter a query\")\n",
    "        else:\n",
    "            drawTilebars(_query,normalized=_norm,sortby=_sortby).display()\n",
    "\n",
    "querybox = widgets.Text(description='Query:')\n",
    "searchbutton = widgets.Button(description=\"Search\")\n",
    "normalizedradio = widgets.RadioButtons(description=\"Normalized?\",options=['true', 'false'])\n",
    "sortradio = widgets.RadioButtons(description=\"Sort by\",options=['name', 'score'])\n",
    "\n",
    "searchbutton.on_click(clicked)\n",
    "normalizedradio.observe(clicked)\n",
    "sortradio.observe(clicked)\n",
    "\n",
    "list_widgets = [widgets.VBox([widgets.HBox([querybox,searchbutton]),\n",
    "                              widgets.HBox([normalizedradio,sortradio])])]\n",
    "accordion = widgets.Accordion(children=list_widgets)\n",
    "accordion.set_title(0,\"Search Controls\")\n",
    "display(accordion,output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "14b706055f04024ef35fdb5ffc86c3aa",
     "grade": false,
     "grade_id": "cell-75ef21e646c2f59f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Additional comments\n",
    "\n",
    "If you think we need to know anything about your solution or design choices, feel free to add details here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mads_information_visualization_ii_v1_assignment4"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
